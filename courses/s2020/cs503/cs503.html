<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>s20cs503</title>

<link href="../../../css/layout.css" rel="stylesheet" type="text/css" />
<style type="text/css">
.holiday {
	color: #808080;
}
</style>
</head>

<body><div id="wrapper">

  <div id="pre-header">
	</div>

  <div id="header">

</div>

<div id="content">


<a name="top"></a>
<h1 align="center">CS503 - Machine Learning- Spring 2020</h1>
<p align="center">&nbsp;</p>
<div align="center">
<table width="600" border="0" align="center">
  <tr>
    <th width="150" scope="col"><em><a href="#courseinfo"> Course Information </a></em></th>
    <th width="150" scope="col"><em><a href ="#grading">Grading Policy</a></em></th>
    <th width="150" scope="col"><em><a href="#lectures">Lectures/Calendar</a></em></th>
    <th width="150" scope="col"><em><a href="#labs">Labs</a></em></th>
    </tr>
</table>
</div>
<p align="center">&nbsp;</p>

<a name="courseinfo"></a>
<section title="Course Information">
<h2 align="left"> Course Information </h2>
<p>&nbsp;</p>

<h3 align="left" class="heading3">Timings and Lecture Hall </h3>
<p><strong>Mondat - 9.00-9.50am</strong></p>
<p><strong>Tuesday - 9.00-9.50am</strong></p>
<p><strong>Wednesday - 9.00-9.50am</strong></p>
<p><strong>Compensatory Lecture Hours - Wednesday 7.00-8.00pm</strong></p>
<p><strong>Ramanujan Block - Room CS1</strong></p>
<h3 align="left" class="heading3">&nbsp; </h3>
<h3 align="left" class="heading3">Description </h3>
<p align="justify">Machine Learning (ML) is the study of computer algorthms that learn and imrpove automatically through experience. ML is an increasingly popular subject due to a wide variety of applications such as autonomous vehicles, hand-written character recognition, automatic speech processing, recommendation systems, etc. This introductory (undergraduate-graduate bridge) course discusses some of the basic and widely used ML techniques, covering a wide range of topics such as  supervised and unsupervised learning, classification and regression, artificial neural networks, and dimensionality reduction. For a comprehensive list of topics covered in the course and course schedule, please see the course calendar. Practical experience will be gained through implementing the ML algorithms for different applications in Python/C/C++. For more details on lab assignments, please see the Labs webpage. This course has a pre-requisite of CSL201 (Data Structures). Background in Linear Algebra, Probability and Statistics, and Optimization will be helpful, though not necessary. This course is supported through the Google TensorFlow Research Award</p>
<p>&nbsp;</p>


<h3 align="left" class="heading3">Reference Material </h3>
<p> There is no fixed textbook for the course. However content will be adopted from the following textbooks</p>
<ul> 
<li><a href="http://www.cs.cmu.edu/~tom/mlbook.html">Machine Learning</a> by Tom Mitchell (ML)</li>
<li><a href="https://mitpress.mit.edu/books/introduction-machine-learning">Introduction to Machine Learning</a> by Ethem Alpaydin (IML)</li>
<li><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning </a> by Trevor Hastie, Robert Tibshirani and Jerome Friedman (available online for free) (ESL)</li>
<li><a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning </a> by Christopher Bishop (PRML)</li>
</ul>
</p>
<p>&nbsp;</p>

<h3 align="left" class="heading3">Instructor Details</h3> 
<p><strong>Narayanan (CK) Chatapuram Krishnan</strong></p>
<p><strong>Office Hours:</strong> Thursday 10.00-11.00am</p>
<p><strong>Office:</strong> 315, SRB</p>
<p><strong>Email:</strong> ckn@iitrpr.ac.in</p>
<p>&nbsp;</p>

<h3 align="left" class="heading3">Teaching Assistants Details</h3> 
<p><strong>Vidhyakamakshi</strong>, <strong>Nikhil Reddy</strong></p><p><strong>Office Hours:</strong> TBD</p>
<p><strong>Office:</strong> Lab 1, First Floor, SRB</p>
<p><strong>Email:</strong> 2017csz005@iitrpr.ac.in, 2018csm1011@iitrpr.ac.in</p>
<p>&nbsp;</p>

<h3 align="left" class="heading3"> Academic Integrity </h3>
<p align="justify">It is expected that students who are taking this course will demonstrate a keen interest in learning and not mere fulfilling the requirement towards their degree. Discussions that help the student understand a concept or a problem is encouraged. However, each student must turn in original work. Plagiarism/copying of any form, will be dealt with strict disciplinary action. This involves, copying from the internet, textbooks and any other material for which you do not own the copyright. Copying part of the code will be considered plagiarism. Lending the code to others will be considered plagiarism too, for it is difficult to investigate who copied whose code. <strong>Students who violate this policy will  directly receive a failing grade in the course.</strong> <strong>Remember - Your partial submission can fetch you some points, but submitting other's work as your own can result in you failing the course</strong>.  Please talk to the instructor if you have questions about this policy. All academic integrity issues will be handled in accordance with institute regulations.</p>
<p>&nbsp;</p>

<div align="right"> 
<a href="#top">Scroll to top </a>
<hr width="50%" />
</div>
</section>

<a name="grading"></a>
<section title="Grading">
<h2 align="left"> Grading Policy </h2>
<p>&nbsp;</p>

<h3 class="heading3"> Grading Policy </h3>
<p> 
<strong>Quizzes:</strong> There will be approximately 4 pre-announced quizzes during the semester. Check the course calendar to learn about dates on which a quiz will be held. All the quiz scores will count towards the student's overall grade. The quizzes will account for 20% of the overall grade.</p>
<p>&nbsp;</p>
<p>
<strong> Labs:</strong> There will be 5 labs. Each lab will have a major programming component and will span for approximately two-three weeks. All the 5 labs will account for 20% of the overall grade. Students having difficulty with the labs are encouraged to contact the TA for assistance.
You are not required to be physically present in the lab during the lab hours. You can complete the labs at your convenience and turn it in by the deadline. There will be penalty for late submission of the labs. It will start at 1% for the first hour after the submission deadline and increase exponentially for every hour hence forth.</p>
<p>&nbsp;</p>
<p><strong>Project:</strong> There will be a course project worth 10% of the overall grade. This will be a group project. The tentaive schedule for the project is as follows
<ul>
<li>Project Proposal (one page outline with the problem statement and references) March 6</li>
<li>Project Review - Week 13</li>
<li>Final Presentation Week 17</li>
</ul></p>
<p>&nbsp;</p>
<p>
<strong> Exams:</strong> The mid and end semester exams together will account for 50% (25% each) of the overall grade.
</p>
<p>&nbsp;</p>
<p><strong>Attendance: </strong>There is no mandatory attendance. However attendance will be taken in every class. This will consitute a bonus of 1% for the final grade and might be helpful for all border line students.</p>
<p>&nbsp;</p>
<p><strong>Passing Critera:</strong> A student must secure an overall score of 40 (out of 100) and a combined score of 60 (out of 200) in the exams to pass the course.</p>
<p>&nbsp;</p>

<h3 class="heading3"> Tentative Grade Breakup* </h3>
<table width="232" border="1">
  <tr>
    <th width="176" scope="row">Quizzes (4)</th>
    <td width="40">20%</td>
  </tr>
  <tr>
    <th scope="row">Labs (5)</th>
    <td>20%</td>
  </tr>
    <tr>
    <th scope="row">Project</th>
    <td>10%</td>
  </tr>
  <tr>
    <th scope="row">Mid-Semester Exam</th>
    <td>25%</td>
  </tr>
    <tr>
    <th scope="row">End-Semester Exam</th>
    <td>25%</td>
  </tr>
    <tr>
    <th scope="row">Total</th>
    <td>100</td>
  </tr>
</table>
<p>*This is a tentative breakup of the grades and can change at the discretion of the instructor. However, any change with respect to the grade break-up will be intimated in advance.</p>
<p>&nbsp;</p>
<p><strong>Grade Sheet:</strong><a href="grades.pdf">PDF</a></p>

<div align="right"> 
<a href="#top">Scroll to top </a>
<hr width="50%" />
</div>
</section>

<a name="lectures"></a>
<section title="Lectures and Calendar">
<h2 align="left"> Lectures and Calendar </h2>
<p align="left">&nbsp;</p>
<h3 class="heading3">Tentative Schedule and List of Topics*</h3>
<div  align="center">
<table width="900" border="1">
  <tr>
    <th width="200" scope="col">Week</th>
    <th width="650" scope="col">Topic and Readings</th>
    <th width="100" scope="col"><div align="center">Quiz/Lab</div></th>
  </tr>
  <tr>
    <th scope="row">1 (Jan6-Jan10)</th>
    <td><p><a href="w1.pdf">Introduction and Supervised Learning</a><br />
   		Readings:
        <ul>
        <li><p>Chapter 1 (ML)</p></li>
        <li><p>Chapter 1 and 2(IML)</p></li>
        </ul>
        </p>
        </td>
    <td><div align="center"></div></td>
  </tr>
  <tr>
      <th scope="row">2 (Jan13-Jan17)</th>
      <td><p><a href="w2.pdf">Decision Tree Learning</a><br />
        Readings:
        <ul>
          <li>
            <p>Chapter 3 (ML)</p>
          </li>
        </ul>
  Other Reference Material: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/decisionForests_MSR_TR_2011_114.pdf">Decision Forests for Classification</a> by Criminisi et al. 2011 (refer Chapters 1-3) <br />
      </p>
      </td>
      <td><div align="center">C1 (Jan 15)</div></td>
  </tr>
  <tr>
   <th scope="row">3 (Jan20-Jan24)</th>
    <td><p><a href="w3.pdf">Linear Regression</a><br />
Readings:
      <ul>
        <li>
          <p><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Notes</a> by Andrew Ng (Part I - 1-5)</p>
        </li>
        <li>
          <p>Chapter 3 (ESL) (for Regularization and LASSO)</p>
        </li>
        <li>
          <p>Chapter 3 - 3.1-3.2 (PRML) (for Bias-Variance Analysis)</p>
        </li>
      </ul>
Other Reference Material:
<ul>
<li><a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">Matrix and Vector Algebra Review</a><br />  </li>
<li><a href="http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Sets and Functions (2nd and 3rd chapter) </a></li>
</ul>
    </p>
    </td>
    <td><div align="center"><a href="q1.pdf">Q1</a> (Jan22)</div></td>
  </tr>
  <tr>
    <th scope="row">4 (Jan27-Jan31)</th>
    <td><p><a href="w4.pdf">Linear Classification</a><br />
Readings:
  <ul>
    <li>
      <p>Chapter 4 - 4.1-4.3 (ESL) (for Classification through Linear Regression, Linear Discriminants and Logistic Regression)</p>
    </li>
    <li>
      <p><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Notes</a> by Andrew Ng (Part II) (for Logistic Regression and Netwon Raphson Method)</p>
    </li>
  </ul>
Other Reference Material: <a href="https://tminka.github.io/papers/logreg/minka-logreg.pdf">Optimizers for Logistic Regression</a><br /></p></td>
    <td><div align="center">C2 (Jan29)</div></td>
  </tr>
  <tr>
   <th scope="row">5 (Feb3-Feb7)</th>
    <td rowspan="2"><p><a href="w5-6.pdf">Artificial Neural Networks</a><br />
Readings:
  <ul>
    <li> Chapter 4 (ML) (for basic perceptron, mlp, and back propagation)</li>
    <li> Chapter 5 - 5.1, 5.5.1-5.5.3 (PRML) (for weight space symmetries and regularization)</li>
    <li> Dropout - <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">Srivastava et. al., 2014</a></li>
    <li>Batch Normalization - <a href="https://arxiv.org/pdf/1502.03167.pdf">Ioffe and Szegedy 2015</a></li>
    <li><a href="http://cs231n.github.io/neural-networks-2/">Training and Tuning a CNN</a></li>
  </ul>
Other Reference Material:
<ul>
  <li>ImageNet Classification with Deep CNN (Alex Net), <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Krizhevsky et. al., 2012</a></li>
  <li><a href="http://www.deeplearningbook.org/">Deep learning book</a>(Computational Graph, Recurrent Network)</li>
</ul><br /></p></td>
    <td><div align="center">C3 (Feb5) </div><div align="center"><a href="l1.zip">L1 </a>(Feb 7)</div></td>
  </tr>
  <tr>
    <th scope="row">6 (Feb10-Feb14)</th>
    <td><div align="center">Q2 (Feb12)</div></td>
  </tr>
  <tr>
     <th scope="row">7 (Feb17-Feb21)</th>
     <td><p><a href="w7.pdf">Experimental Design</a><br />
Readings:
  <ul>
    <li> Chapter 4 (ML) (for interval estimation and hypothesis testing) </li>
    <li> Chapter 19.6-19.11 (IML) (for measures of performance, design of experiments, and hypothesis testing) </li>
  </ul>
Other Reference Material: </p> </td>
    <td><div align="center">C4 (Feb19)</div><div align="center"><a href="l2.zip">L2 </a>(Feb21)</div></td>
  </tr>
  <tr class="holiday">
    <th scope="row">8 (Feb24-Feb28)</th>
    <td><a href="mid.pdf">mid</a> midsol</td>
    <td><div align="center"></div></td>
  </tr>
  <tr>
     <th scope="row">9 (Mar2-Mar6)</th>
    <td rowspan="3"><p><a href="w9-10.pdf">Kernel Methods</a><br />
Readings:
  <ul>
    <li> Chapter 13 (IML)</li>
    <li> <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">Notes</a> by Andrew Ng</li>
    <li> <a href="smo-original.pdf">SMO</a> by Platt </li>
  </ul>
Other Reference Material:
<ul>
  <li>Lagrange Multipliers Appendix E (PRML)</li>
  <li>Lectures 12.1-12.6 of Andrew Ng on coursera (to get a quick overview)</li>
  <li><a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">LibSVM</a> - popular SVM toolbox</li>
</ul><br /></p>
    </td>
    <td><div align="center">C5 (Mar4)</div></td>
  </tr>
  <tr class="holiday">
    <th scope="row">10 (Mar9-Mar13)</th>
    <td><div align="center"></div></td>
  </tr>
  <tr>
     <th scope="row">11 (Mar16-Mar20)</th>
     <td><div align="center"><a href="l3.zip">L3 </a>(Mar20)</div><div align="center">C6 (Mar18)</div></td>
  </tr>
  <tr>
      <th scope="row">12 (Mar23-Mar27)</th>
     <td><p><a href="w11.pdf">Clustering</a><br />
         Readings:
         <ul>
         <li> Chapter 14 - 14.3 (ESL) (for k-Means, Hierarchical Clustering)</li>
         <li> Chapter 9 - 9.1-9.2 (PRML) (for Gaussian Mixture Models)</li>
         </ul>
         Other Reference Material:
         <ul>
         <li> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.9220">DBSCAN</a> </li>
         </ul></p></td>
     <td><div align="center">C7 (Mar25)</div><div align="center"><a href="l4.zip">L4 </a>(Mar27)</div></td>
   </tr>
   <tr>
      <th scope="row">13 (Mar30-Apr3)</th>
     <td><p><a href="w12.pdf">Dimensionality Reduction</a><br />
         Readings:
         <ul>
         <li> Chapter 14.5 (ESL) (Principal Component Analysis)</li>
         <li> Chapter 12.1 (PRML)</li>
         <li> <a href="kernelPCA_scholkopf.pdf">Kernel PCA (original paper)</a></li>
         <li> Chapter 6.6 (IML) (LDA)</li>
         </ul>
         Other Reference Material:
         <ul>
             <li><a href="http://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-PCA.pdf">Kernel PCA (notes)</a></li>
             <li>LDA - Chapter  5.7-5.8 - Pattern Classification by Duda, Hart and Stork</li>
             <li>Non-linear dimensionality reduction <a href="http://web.mit.edu/cocosci/isomap/isomap.html">ISOMAP</a></li>
         </ul></p></td>
     <td><div align="center">Q3 (Apr1)</div></td>
   </tr>
  <tr>
   <th scope="row">14 (Apr6-Apr10)</th>
    <td><p><a href="w14-15.pdf">Bayesian Learning</a><br />
Readings:
      <ul>
        <li> Chapter 6 (ML)</li>
      </ul>
Other Reference Material:
     </p></td>
    <td><div align="center">C8 (Apr8)</div><div align="center"><a href="l5.zip">L5 </a>(Apr10)</div></td>
  </tr>
  <tr>
    <th scope="row">15 (Apr13-Apr17)</th>
    <td><p><a href="w13-14.pdf">Bayesian Networks</a> <br />
    Readings:
      <ul>
        <li> Chapter 14 (AIMA - Russell and Norvig)</li>
      </ul>
      </p></td>
    <td><div align="center">Q4 (Apr15)</div></td>
  </tr>
  <tr>
    <th scope="row">16 (Apr20-Apr24) (3)</th>
      <td><p><a href="w15.pdf">Hidden Markov Models</a><br />
        Readings:
        <ul>
          <li> <a href="rabiner.pdf">Tutorial Paper on HMM by Rabiner, 1999</a> (until Page 266) </li>
        </ul>
Other Reference Material:
<ul>
  <li> Chapter 13.1-13.2 (PRML) </li>
</ul></p></td>
    <td></td>
  </tr>
  <tr>
    <th scope="row">17 (Apr29-May3)</th>
    <td><p><a href="w16.pdf">Concluding Remarks and Project Presentations</a><br /></p>
    <td></td>
  </tr>
  <tr class="holiday">
   <th scope="row">18 (May6-May10)</th>
    <td><a href="end.pdf">end</a><br /></td>
    <td><div align="center"></div></td>
  </tr>
 
</table>
</div>
<p>*This is a tentative list of topics that will be covered during the semester. The topics and schedule can change according to the need at the discretion of the instructor. </p>
<p>ML - Machine Learning</p>
<p>PRML - Pattern Recognition and Machine Learning</p>
<p>IML - Introduction to Machine Learning</p>
<p> ESL - Elements of Statistical Learning</p>
<div align="right"> 
  <a href="#top">Scroll to top </a>
  <hr width="50%" />
</div>
</section>

<a name="labs"></a>
<section title="Labs">
<h2 align="left"> Labs </h2>
<ul>
  <li><a href="l1.zip">Lab 1</a> - due 11.55pm Friday February 7th 2020</li>
  <li><a href="l2.zip">Lab 2 - </a>due 11.55pm  Friday February 21st 2020</li>
  <li><a href="l3.zip">Lab 3 - </a> due 11.55pm Monday March 16th 2020</li>
  <li><a href="l4.zip">Lab 4 - </a> due 11.55pm Friday March 27th 2020</li>
  <li><a href="l5.zip">Lab 5 - </a> due 11.55pm Friday April 10th 2020</li>
</ul>
<p align="left">&nbsp;</p>
<div align="right"> 
  <a href="#top">Scroll to top </a>
  <hr width="50%" />
</div>
</section>

</div>

<div id="footer">
<p>Narayanan (CK) Chatapuram Krishnan - ckn@iitrpr.ac.in - Indian Institute of Technology Ropar</p>
</div>

</html>
